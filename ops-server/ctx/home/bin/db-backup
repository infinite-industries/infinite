#!/usr/bin/env sh
#
# Backup a Postgres database and optionally copy it to an S3 bucket.
#
# Requirements: pg_dump aws
#
# Copyright Jason Swank <jswank@scalene.net>
#
######
set -o errexit
set -o nounset

usage(){
  cat <<EOF
Backup a Postgres database and optionally copy it to an S3 bucket.

Usage: db-backup [flags] [<database>]

Parameters:
  <database>              name of the database.  

Flags:
  -h                      display this help
  -n                      dry run: just print actions that would happen

Environment:
  BACKUP_DIR   Local directory to write backups
  NUM_BACKUPS  Number of backups to retain locally. Default: 4
  S3_BUCKET    Name of the S3 bucket. If unset, an S3 copy will not occur
  S3_PATH      S3 path to write backups. The path requires a trailing slash. Default: /.
  PGDATABASE
  PGHOST
  PGPORT
  PGUSER
  PGPASSWORD
  AWS*

Description: 

This script is meant to be run via cron on a periodic (daily) basis.  It uses
pg_dump to create a backup of a postgres database.  

Each backup will have a name like <database>-<date>:
  database - PGDATABASE
  date     - date -u +%Y-%m-%dT%H:%M:%S (2023-02-25T22:42:16Z)

A symlink, latest, points to the latest backup.

NUM_BACKUPS (default 10) will be retained.

EOF
}


# set defaults for vars that are required
PGDATABASE=${PGDATABASE:=""}
BACKUP_DIR=${BACKUP_DIR:=/var/tmp/backups}
S3_BUCKET=${S3_BUCKET:=""}
S3_PATH=${S3_PATH:=""}
# needs to be exported for perl subshell
export NUM_BACKUPS="${NUM_BACKUPS:=4}"
DRYRUN=0          

fail() {
  printf '%s\n' "$1" >&2
  exit 1
}

init() {

  OPTIND=1

  while getopts "hn?" opt; do
      case "$opt" in
      h|\?)
        usage
        exit 0
        ;;
      n)
        DRYRUN=1
        ;;
      esac
  done

  shift $((OPTIND-1))

  if [ $# -gt 0 ]; then
    PGDATABASE="$1"
  fi

  if [ -z "${PGDATABASE}" ]; then
    fail "A database was not specified via environment or as a parameter"
  fi

  
}

main() {
  # run the init code
  init "$@"

  # the guts of the script
 
  # create the backup directory if required
  if [ ! -d "${BACKUP_DIR}" ]; then
    if [ $DRYRUN -eq 0 ]; then 
      mkdir "${BACKUP_DIR}" || fail "unable to create directory ${BACKUP_DIR}"
    fi
  elif [ ! -w "${BACKUP_DIR}" ]; then
    fail "${BACKUP_DIR} is not writable"
  fi
 
  backup_file="${PGDATABASE}.$(date -u +%Y-%m-%dT%H:%M:%SZ)"

  if [ ${DRYRUN} -eq 1 ]; then
    printf 'pg_dump -d "sslmode=require" -Fc -f %s\n' "${BACKUP_DIR}/${backup_file}"
    printf 'ln -sf %s %s\n' "${backup_file}" "${BACKUP_DIR}/${PGDATABASE}.latest"
  else
    pg_dump -d "sslmode=require" -Fc -f "${BACKUP_DIR}/${backup_file}"
    ln -sf "${backup_file}" "${BACKUP_DIR}/${PGDATABASE}.latest"
  fi

  # remove backup files in excess of the limit
  #  sort -r => sorts list newest to oldest
  #  perl '' => print line if line number greater than NUM_BACKUPS 
  if [ ${DRYRUN} -eq 1 ]; then                           
    find "${BACKUP_DIR}" -name "${PGDATABASE}.*" -print \
      | sort -r \
      | perl -nE 'print if $. >$ENV{q/NUM_BACKUPS/}' \
      | xargs
  else                                                                 
    find "${BACKUP_DIR}" -name "${PGDATABASE}.*" -print \
      | sort -r \
      | perl -nE 'print if $. >$ENV{q/NUM_BACKUPS/}' \
      | xargs rm -f
  fi

  # create a symlink to the most recent file
  most_recent_file=$(find ${BACKUP_DIR} -name "${PGDATABASE}*" -print | sort | head -1)
  if [[ ${DRYRUN} -eq 1 ]]; then
    echo "ln -sf ${most_recent_file} latest"
  else
    ln -sf ${most_recent_file} latest
  fi

  if [ -z "${S3_BUCKET}" ]; then
    exit
  fi

  # copy files to S3
  if [ ${DRYRUN} -eq 1 ]; then
    printf 'aws s3 cp %s %s\n' "${BACKUP_DIR}/${backup_file}" "backups:${S3_BUCKET}/${S3_PATH}/${backup_file}"
  else
    aws s3 cp "${BACKUP_DIR}/${backup_file}" "s3://${S3_BUCKET}/${S3_PATH}/${backup_file}" >/dev/null
  fi

}

main "$@"
